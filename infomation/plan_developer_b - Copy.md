# Developer B ‚Äî AI/ML & Automation Features

> **–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è:** Whisper –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è, AI Director, Smart Editing, Fusion  
> **–§–∞–π–ª—ã:** `src/api/ai_director.py`, `src/api/whisper_node.py`, `src/api/smart_editing.py`, `config/`

---

## üî¥ –§–∞–∑–∞ 1: –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è (–î–µ–Ω—å 1-2)

### –ó–∞–¥–∞—á–∞ B1.1: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Whisper
**–§–∞–π–ª—ã:**
- [MODIFY] [whisper_node.py](file:///c:/GenModels/[Antigravity]/projects/test1/davinci-resolve-mcp/src/api/whisper_node.py)
- [NEW] `config/whisper_config.json`

**–ü—Ä–æ–±–ª–µ–º–∞:** –ó–∞—Ö–∞—Ä–¥–∫–æ–∂–µ–Ω–Ω—ã–µ –ø—É—Ç–∏:
```python
WHISPER_VENV_PYTHON = r"c:\GenModels\Whisper-WebUI\venv\Scripts\python.exe"
WHISPER_GEN_SCRIPT = r"C:\Users\Black\.gemini\whisper_to_json.py"
```

**–†–µ—à–µ–Ω–∏–µ:**

1. –°–æ–∑–¥–∞—Ç—å `config/whisper_config.json`:
```json
{
  "whisper_python": "",
  "whisper_script": "",
  "model_size": "large-v3",
  "use_native_resolve_ai": true,
  "cache_transcriptions": true,
  "fallback_to_system_whisper": true
}
```

2. –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å `whisper_node.py`:
```python
import json
from pathlib import Path

def get_whisper_config() -> dict:
    """Load Whisper configuration with fallbacks."""
    config_path = Path(__file__).parent.parent.parent / "config" / "whisper_config.json"
    
    defaults = {
        "whisper_python": os.environ.get("WHISPER_PYTHON", "python"),
        "whisper_script": os.environ.get("WHISPER_SCRIPT", ""),
        "model_size": "large-v3",
        "use_native_resolve_ai": True,
        "fallback_to_system_whisper": True
    }
    
    if config_path.exists():
        try:
            with open(config_path, encoding="utf-8") as f:
                user_config = json.load(f)
                return {**defaults, **user_config}
        except Exception as e:
            logger.warning(f"Failed to load whisper config: {e}")
    
    return defaults

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
config = get_whisper_config()
WHISPER_PYTHON = config["whisper_python"]
WHISPER_SCRIPT = config["whisper_script"]
```

---

### –ó–∞–¥–∞—á–∞ B1.2: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å parse_ai_segments()
**–§–∞–π–ª:** [ai_director.py](file:///c:/GenModels/[Antigravity]/projects/test1/davinci-resolve-mcp/src/api/ai_director.py)

**–ü—Ä–æ–±–ª–µ–º–∞:** –§—É–Ω–∫—Ü–∏—è –ø—É—Å—Ç–∞—è (—Ç–æ–ª—å–∫–æ `pass`)

**–†–µ—à–µ–Ω–∏–µ ‚Äî –ø–æ–¥–¥–µ—Ä–∂–∫–∞ 3 —Ñ–æ—Ä–º–∞—Ç–æ–≤:**

```python
import re
import json
from typing import List, Dict, Any

def parse_ai_segments(ai_selection_text: str, whisper_data: dict) -> List[Dict[str, Any]]:
    """Parse AI-suggested segments from multiple formats.
    
    Supported formats:
    1. JSON array: [{"start": 12.5, "end": 45.0, "title": "Hook"}]
    2. Text lines: "Reel 1: 12.5 - 45.0 (Hook about coding)"
    3. Segment indices: "Use segments: 0, 3, 5-8, 12"
    
    Args:
        ai_selection_text: Raw text from AI assistant
        whisper_data: Whisper transcription JSON for index resolution
        
    Returns:
        List of segment dicts with start, end, title
    """
    segments = []
    
    # === Format 1: JSON ===
    try:
        # Try parsing as JSON array
        text = ai_selection_text.strip()
        if text.startswith('['):
            parsed = json.loads(text)
            if isinstance(parsed, list):
                for item in parsed:
                    if "start" in item and "end" in item:
                        segments.append({
                            "start": float(item["start"]),
                            "end": float(item["end"]),
                            "title": item.get("title", f"Segment {len(segments)+1}")
                        })
                if segments:
                    return segments
    except json.JSONDecodeError:
        pass
    
    # === Format 2: Text "Reel N: start - end (description)" ===
    reel_pattern = r"(?:Reel\s*\d*:?)?\s*(\d+\.?\d*)\s*[-‚Äì‚Äî]\s*(\d+\.?\d*)\s*(?:s(?:ec)?)?(?:\s*\(([^)]*)\))?"
    matches = re.findall(reel_pattern, ai_selection_text, re.IGNORECASE)
    
    for match in matches:
        start_time = float(match[0])
        end_time = float(match[1])
        title = match[2].strip() if match[2] else f"Segment {len(segments)+1}"
        
        if end_time > start_time:
            segments.append({
                "start": start_time,
                "end": end_time,
                "title": title
            })
    
    if segments:
        return segments
    
    # === Format 3: Segment indices "0, 3, 5-8" ===
    whisper_segments = whisper_data.get("segments", []) if whisper_data else []
    
    if whisper_segments:
        # Match patterns like "segments: 0, 3, 5-8" or just "0, 3, 5-8"
        index_text = re.sub(r"(?:use\s+)?segments?\s*:?\s*", "", ai_selection_text, flags=re.IGNORECASE)
        index_pattern = r"(\d+)(?:\s*[-‚Äì‚Äî]\s*(\d+))?"
        
        for match in re.findall(index_pattern, index_text):
            start_idx = int(match[0])
            end_idx = int(match[1]) if match[1] else start_idx
            
            for idx in range(start_idx, min(end_idx + 1, len(whisper_segments))):
                seg = whisper_segments[idx]
                segments.append({
                    "start": float(seg.get("start", 0)),
                    "end": float(seg.get("end", 0)),
                    "title": seg.get("text", "")[:50].strip()
                })
    
    return segments
```

---

### –ó–∞–¥–∞—á–∞ B1.3: Fallback –Ω–∞ Native Resolve AI
**–§–∞–π–ª:** `whisper_node.py`

–î–æ–±–∞–≤–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–≥–æ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ç–æ—Ä–∞ DaVinci:

```python
def transcribe_with_native_ai(resolve, clip_name: str) -> dict:
    """Fallback: Use DaVinci Resolve's built-in transcription."""
    project = resolve.GetProjectManager().GetCurrentProject()
    media_pool = project.GetMediaPool()
    
    # Find clip
    clip = find_clip_by_name(media_pool, clip_name)
    if not clip:
        return {"error": f"Clip not found: {clip_name}"}
    
    # Trigger transcription (Resolve 18.5+)
    if hasattr(clip, 'TranscribeAudio'):
        result = clip.TranscribeAudio()
        if result:
            # Get transcription text
            transcription = clip.GetMetadata("Transcription")
            return {"text": transcription, "source": "native_resolve_ai"}
    
    return {"error": "Native transcription not available"}
```

---

## üü° –§–∞–∑–∞ 2: –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ (–î–µ–Ω—å 3-5)

### –ó–∞–¥–∞—á–∞ B2.1: –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ whisper_node.py
**–¶–µ–ª—å:** –°–¥–µ–ª–∞—Ç—å –º–æ–¥—É–ª—å —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–º

```python
class WhisperProvider:
    """Abstract base for transcription providers."""
    
    def transcribe(self, file_path: str) -> dict:
        raise NotImplementedError

class ExternalWhisperProvider(WhisperProvider):
    """Uses external Whisper installation."""
    pass

class NativeResolveProvider(WhisperProvider):
    """Uses DaVinci Resolve Neural Engine."""
    pass

class OpenAIWhisperProvider(WhisperProvider):
    """Uses OpenAI Whisper API (cloud)."""
    pass

def get_transcription_provider(config: dict) -> WhisperProvider:
    """Factory for transcription providers."""
    if config.get("use_native_resolve_ai"):
        return NativeResolveProvider()
    elif config.get("openai_api_key"):
        return OpenAIWhisperProvider(config["openai_api_key"])
    else:
        return ExternalWhisperProvider(config)
```

---

### –ó–∞–¥–∞—á–∞ B2.2: –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ jump_cut.py
–£–ª—É—á—à–∏—Ç—å –∞–ª–≥–æ—Ä–∏—Ç–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–∏—à–∏–Ω—ã:

```python
def generate_jump_cut_edits(
    whisper_data: dict, 
    clip_name: str, 
    silence_threshold: float = 0.5,
    min_speech_duration: float = 0.3,  # NEW
    add_handles: float = 0.1           # NEW
) -> List[dict]:
    """Generate edit points removing silence."""
    pass
```

---

### –ó–∞–¥–∞—á–∞ B2.3: –î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è AI —Ñ—É–Ω–∫—Ü–∏–π
–°–æ–∑–¥–∞—Ç—å `docs/AI_FEATURES.md`:
- –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç Whisper –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è
- –§–æ—Ä–º–∞—Ç—ã –¥–ª—è parse_ai_segments
- –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–æ–º–ø—Ç–æ–≤ –¥–ª—è AI Director

---

## üü¢ –§–∞–∑–∞ 3: –ù–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ (–ù–µ–¥–µ–ª—è 1-2)

### –ó–∞–¥–∞—á–∞ B3.1: Smart Reframe
**–§–∞–π–ª:** [smart_editing.py](file:///c:/GenModels/[Antigravity]/projects/test1/davinci-resolve-mcp/src/api/smart_editing.py)

```python
def smart_reframe(
    resolve, 
    clip_name: str, 
    aspect_ratio: str = "9:16",
    tracking_target: str = "face"  # "face", "body", "center"
) -> str:
    """–£–º–Ω–æ–µ –∫–∞–¥—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ–º –æ–±—ä–µ–∫—Ç–∞.
    
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç DaVinci Neural Engine –¥–ª—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø–æ–∑–∏—Ü–∏–∏ –ª–∏—Ü–∞/–æ–±—ä–µ–∫—Ç–∞
    –∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–∞–¥—Ä–∏—Ä–æ–≤–∞–Ω–∏—è –ø–æ–¥ –≤–µ—Ä—Ç–∏–∫–∞–ª—å–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç.
    """
    project = resolve.GetProjectManager().GetCurrentProject()
    timeline = project.GetCurrentTimeline()
    
    # –ù–∞–π—Ç–∏ –∫–ª–∏–ø –Ω–∞ —Ç–∞–π–º–ª–∞–π–Ω–µ
    clip = find_timeline_item(timeline, clip_name)
    if not clip:
        return f"Clip not found on timeline: {clip_name}"
    
    # –í–∫–ª—é—á–∏—Ç—å Smart Reframe (Resolve 17+)
    clip.SetProperty("SmartReframe", 1)
    clip.SetProperty("SmartReframeAspect", aspect_ratio)
    
    if tracking_target == "face":
        clip.SetProperty("SmartReframeTrackFace", 1)
    
    return f"Smart Reframe applied to {clip_name} ({aspect_ratio})"
```

---

### –ó–∞–¥–∞—á–∞ B3.2: Auto Subtitles
```python
def auto_subtitle(
    resolve, 
    timeline_name: str = None,
    style: str = "default",
    position: str = "bottom"
) -> str:
    """–°–æ–∑–¥–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã –∏–∑ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏–∏.
    
    Args:
        style: "default", "tiktok", "youtube", "cinematic"
        position: "bottom", "center", "top"
    """
    project = resolve.GetProjectManager().GetCurrentProject()
    timeline = project.GetCurrentTimeline()
    
    # –ü–æ–ª—É—á–∏—Ç—å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—é —Å —Ç–∞–π–ª–∞–π–Ω–∞
    # –°–æ–∑–¥–∞—Ç—å —Å—É–±—Ç–∏—Ç—Ä—ã —á–µ—Ä–µ–∑ Fusion Text+
    pass
```

---

### –ó–∞–¥–∞—á–∞ B3.3: Scene Detection
```python
def detect_scenes(
    resolve, 
    clip_name: str, 
    sensitivity: float = 0.5
) -> List[Dict]:
    """–û–ø—Ä–µ–¥–µ–ª–∏—Ç—å —Ç–æ—á–∫–∏ —Å–º–µ–Ω—ã —Å—Ü–µ–Ω.
    
    Returns:
        List of {"frame": int, "timecode": str, "confidence": float}
    """
    # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DaVinci Scene Cut Detection
    pass
```

---

### –ó–∞–¥–∞—á–∞ B3.4: Music Sync (Beat Detection)
```python
def auto_music_sync(
    resolve, 
    video_clips: List[str], 
    music_clip: str,
    sync_mode: str = "beat"  # "beat", "bar", "phrase"
) -> str:
    """–°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–∞—Ä–µ–∑–∫—É –≤–∏–¥–µ–æ –ø–æ–¥ —Ä–∏—Ç–º –º—É–∑—ã–∫–∏.
    
    1. –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç BPM –º—É–∑—ã–∫–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–µ–∫–∞
    2. –°–æ–∑–¥–∞–µ—Ç –º–∞—Ä–∫–µ—Ä—ã –Ω–∞ –∫–∞–∂–¥—ã–π –±–∏—Ç
    3. –ù–∞—Ä–µ–∑–∞–µ—Ç –≤–∏–¥–µ–æ –ø–æ –º–∞—Ä–∫–µ—Ä–∞–º
    """
    pass
```

---

### –ó–∞–¥–∞—á–∞ B3.5: Fusion Operations
**–§–∞–π–ª:** [NEW] `src/api/fusion_operations.py`

```python
def add_text_overlay(
    resolve, 
    text: str, 
    position: tuple = (0.5, 0.1),
    font: str = "Arial",
    size: int = 72,
    color: str = "#FFFFFF",
    duration: float = None  # None = entire clip
) -> str:
    """–î–æ–±–∞–≤–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ–≤–µ—Ä–ª–µ–π —á–µ—Ä–µ–∑ Fusion."""
    pass

def create_lower_third(
    resolve,
    name: str,
    title: str,
    subtitle: str = "",
    template: str = "modern"  # "modern", "minimal", "news"
) -> str:
    """–°–æ–∑–¥–∞—Ç—å –ø–ª–∞—à–∫—É —Å –∏–º–µ–Ω–µ–º/—Ç–∏—Ç—É–ª–æ–º."""
    pass

def apply_transition_effect(
    resolve,
    effect: str = "fade",  # "fade", "zoom", "slide", "glitch"
    duration: int = 30  # frames
) -> str:
    """–ü—Ä–∏–º–µ–Ω–∏—Ç—å –ø–µ—Ä–µ—Ö–æ–¥ –º–µ–∂–¥—É –∫–ª–∏–ø–∞–º–∏."""
    pass
```

---

## üìã –§–∞–∑–∞ 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –ó–∞–¥–∞—á–∞ B4.1: Unit Tests –¥–ª—è AI –º–æ–¥—É–ª–µ–π
**–§–∞–π–ª—ã:**
- `tests/unit/test_ai_director.py`
- `tests/unit/test_whisper_node.py`
- `tests/unit/test_jump_cut.py`

```python
# test_ai_director.py
import pytest
from src.api.ai_director import parse_ai_segments, prepare_transcription_for_ai

class TestParseAiSegments:
    
    def test_parse_json_format(self):
        text = '[{"start": 0, "end": 10, "title": "Intro"}]'
        result = parse_ai_segments(text, {})
        assert len(result) == 1
        assert result[0]["start"] == 0
        assert result[0]["end"] == 10
    
    def test_parse_text_format(self):
        text = "Reel 1: 12.5 - 45.0 (Hook about coding)"
        result = parse_ai_segments(text, {})
        assert len(result) == 1
        assert result[0]["start"] == 12.5
        assert result[0]["end"] == 45.0
        assert "Hook" in result[0]["title"]
    
    def test_parse_segment_indices(self, sample_whisper_data):
        text = "Use segments: 0, 2"
        result = parse_ai_segments(text, sample_whisper_data)
        assert len(result) == 2
    
    def test_empty_input(self):
        result = parse_ai_segments("", {})
        assert result == []
```

---

### –ó–∞–¥–∞—á–∞ B4.2: Mock –¥–ª—è Whisper
**–§–∞–π–ª:** `tests/mocks/whisper_mock.py`

```python
def mock_whisper_transcription(file_path: str) -> dict:
    """Mock transcription for testing without actual Whisper."""
    return {
        "text": "This is a mock transcription for testing purposes.",
        "segments": [
            {"start": 0.0, "end": 2.0, "text": "This is a mock"},
            {"start": 2.5, "end": 5.0, "text": "transcription for testing"},
            {"start": 10.0, "end": 12.0, "text": "purposes."}
        ],
        "language": "en"
    }
```

---

## –ß–µ–∫–ª–∏—Å—Ç Developer B

### –§–∞–∑–∞ 1
- [ ] B1.1: –°–æ–∑–¥–∞—Ç—å config/whisper_config.json
- [ ] B1.1: –ú–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞—Ç—å whisper_node.py –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥–∞
- [ ] B1.2: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å parse_ai_segments() ‚Äî JSON —Ñ–æ—Ä–º–∞—Ç
- [ ] B1.2: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å parse_ai_segments() ‚Äî —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–æ—Ä–º–∞—Ç
- [ ] B1.2: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å parse_ai_segments() ‚Äî –∏–Ω–¥–µ–∫—Å—ã —Å–µ–≥–º–µ–Ω—Ç–æ–≤
- [ ] B1.3: –î–æ–±–∞–≤–∏—Ç—å fallback –Ω–∞ Native Resolve AI

### –§–∞–∑–∞ 2
- [ ] B2.1: –†–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥ whisper_node.py (providers)
- [ ] B2.2: –£–ª—É—á—à–∏—Ç—å jump_cut.py (min_duration, handles)
- [ ] B2.3: –°–æ–∑–¥–∞—Ç—å docs/AI_FEATURES.md

### –§–∞–∑–∞ 3
- [ ] B3.1: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å smart_reframe()
- [ ] B3.2: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å auto_subtitle()
- [ ] B3.3: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å detect_scenes()
- [ ] B3.4: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å auto_music_sync()
- [ ] B3.5: –°–æ–∑–¥–∞—Ç—å fusion_operations.py
- [ ] B3.5: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å add_text_overlay()
- [ ] B3.5: –†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å create_lower_third()

### –§–∞–∑–∞ 4
- [ ] B4.1: Unit —Ç–µ—Å—Ç—ã –¥–ª—è ai_director.py
- [ ] B4.1: Unit —Ç–µ—Å—Ç—ã –¥–ª—è whisper_node.py
- [ ] B4.1: Unit —Ç–µ—Å—Ç—ã –¥–ª—è jump_cut.py
- [ ] B4.2: –°–æ–∑–¥–∞—Ç—å whisper_mock.py
